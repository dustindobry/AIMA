{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cfe643e",
      "metadata": {
        "id": "8cfe643e"
      },
      "source": [
        "# Chapter 1 - ML Basics Lab II — Training a Simple MLP in NumPy\n",
        "\n",
        "This notebook demonstrates how to build and train a simple Multi-Layer Perceptron (MLP) from scratch using NumPy.  \n",
        "We will use a synthetic 2D dataset (`make_moons`) so that we can easily **visualize the decision boundary**.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand how forward and backward passes work in a neural network\n",
        "- Implement **Stochastic Gradient Descent (SGD)** manually\n",
        "- Explore the effect of the learning rate on training dynamics\n",
        "- Visualize both the **loss curves** and the **decision boundary evolution**\n",
        "\n",
        "---\n",
        "\n",
        "**This week's exercise has 7 tasks for a total of 8 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "What we are trying to accomplish in this notebook is to manually build the forward and backward passes for training a model using Stochastic Gradient Descent (SGD), which we learned about in the most recent lecture. For now, we continue using Numpy to implement this process, using an extremely small model. We will switch to the more modern PyTorch in the coming weeks, and automate these calculations away - an extremely useful feature, as you will no doubt agree after this exercise.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7399f089",
      "metadata": {
        "id": "7399f089"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ada696d",
      "metadata": {
        "id": "2ada696d"
      },
      "source": [
        "## 2. Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ifWbT6d6hk2L",
      "metadata": {
        "id": "ifWbT6d6hk2L"
      },
      "source": [
        "We start by making ourselves a little toy dataset called \"Moons\". In essence, the dataset has two characteristic values, let's call them $X_{1}$ and $X_{2}$, and a class $y$.\n",
        "\n",
        "**Task 1 (1 point)**: Get a feel for our dataset by making a scatterplot of all the data points. To differentiate the classes, have data points be colored according to their class.\n",
        "\n",
        "If you see two distinct groups which look slightly banana-shaped, or a bit like a yin-yang symbol, you've done it right. The train and test datasets should look different, but very similar.\n",
        "\n",
        "**Task 2 (1 point)**: Is a simpler, older model like a decision tree or a logistic regression likely to be able to differentiate classes in our dataset well? Explain why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1001e5f",
      "metadata": {
        "id": "e1001e5f"
      },
      "outputs": [],
      "source": [
        "# Generate 2D moons dataset\n",
        "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Plot\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e99b8e9",
      "metadata": {
        "id": "0e99b8e9"
      },
      "source": [
        "## 3. Define MLP Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zq_uhphljxNm",
      "metadata": {
        "id": "Zq_uhphljxNm"
      },
      "source": [
        "The next step is to build our Multi-layer Perceptron. While they can be arbitrarily large, for our toy dataset a small one with just two layers, a ReLU function, and a Sigmoid at the end will suffice. To do this, we will need a few functions, which are for you to implement.\n",
        "\n",
        "**Task 3 (1 point)**: Create a Sigmoid function, a ReLU function, a ReLU derivative function, and a cross-entropy loss function.\n",
        "\n",
        "**Task 4 (1 point)**: Create a Forward function with inputs $X$, and weights and biases for the two layers of our MLP, $W_1$, $b_1$, $W_2$ and $b_2$. As outputs, the function should obviously give back the output of our MLP, but we also want the output of all the steps inbetween, namely $X$, $z_1$ (the output after the first layer), $h_1$ (the output after the first layer and the ReLU function), and $z_2$ (the output after the second layer, but before the sigmoid). Having this information will be necessary to later compute our gradients.\n",
        "\n",
        "**Task 5 (1 point)**: Can you think of a reason why we give the model parameters a starting value (initialization) that is different from all zeroes? Elaborate what would happen if we did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ba5c99",
      "metadata": {
        "id": "08ba5c99"
      },
      "outputs": [],
      "source": [
        "# We build a **2-layer MLP**:\n",
        "# - Input → Hidden (ReLU) → Output (Sigmoid)\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(z):\n",
        "    # Write me :)\n",
        "    pass\n",
        "\n",
        "def relu(z):\n",
        "    # Write me :)\n",
        "    pass\n",
        "\n",
        "def drelu(z):\n",
        "    # Write me :)\n",
        "    pass\n",
        "\n",
        "# Loss function (cross-entropy (binary))\n",
        "def compute_loss(y_pred, y_true):\n",
        "    # Write me :)\n",
        "    pass\n",
        "\n",
        "# Initialize weights - We start with a model that has completely random parameters\n",
        "def init_weights(input_dim, hidden_dim, output_dim):\n",
        "    np.random.seed(0)\n",
        "    W1 = 0.1 * np.random.randn(input_dim, hidden_dim)\n",
        "    b1 = np.zeros((1, hidden_dim))\n",
        "    W2 = 0.1 * np.random.randn(hidden_dim, output_dim)\n",
        "    b2 = np.zeros((1, output_dim))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Forward pass\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    # Write me :)\n",
        "    y = ...\n",
        "    cache = (X, z1, h1, z2, out)\n",
        "    return y, cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72196c8",
      "metadata": {
        "id": "e72196c8"
      },
      "source": [
        "## 4. Gradient Descent\n",
        "\n",
        "**Task 6 (2 points)**: Next, implement a single step of stochastic gradient descent, using the functions you just made. Most code is already filled in, except for the gradients. These are for you to compute.\n",
        "\n",
        "1. Forward pass - Compute the model predictions, using the forward function you just defined\n",
        "2. Loss - Compute the loss, using the loss function you just defined\n",
        "3. Backward pass - Compute the gradients for $W_1$, $b_1$, $W_2$, and $b_2$. Each of these is a full derivative of the Loss with respect to the parameter in question. This involves multiple applications of the chain rule. We kept track of the values of the intermediate calculation steps for this purpose.\n",
        "4. Apply SGD update - Subtract $lr * gradient$ from each parameter.\n",
        "5. Return updated parameters and current loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af95b92",
      "metadata": {
        "id": "6af95b92"
      },
      "outputs": [],
      "source": [
        "# --- Gradient computation + SGD update ---\n",
        "def step(W1, b1, W2, b2, X, y, lr=0.1):\n",
        "    \"\"\"\n",
        "    Perform one optimization step with SGD.\n",
        "\n",
        "    Inputs:\n",
        "        W1, b1, W2, b2 : current weights and biases\n",
        "        X : input data batch, shape (N, d)\n",
        "        y : labels, shape (N, 1)\n",
        "        lr : learning rate (float)\n",
        "\n",
        "    Returns:\n",
        "        Updated weights W1, b1, W2, b2\n",
        "        Current loss (float)\n",
        "    \"\"\"\n",
        "\n",
        "    # Forward\n",
        "    y_pred, cache = forward(X, W1, b1, W2, b2)\n",
        "\n",
        "    # Loss\n",
        "    loss = compute_loss(y_pred, y)\n",
        "\n",
        "    # Backward (gradients)\n",
        "    X_, z1, h1, z2, out = cache\n",
        "    m = X_.shape[0]\n",
        "\n",
        "    dz2 = ...\n",
        "    dW2 = ...\n",
        "    db2 = ...\n",
        "\n",
        "    dh1 = ...\n",
        "    dz1 = ...\n",
        "    dW1 = ...\n",
        "    db1 = ...\n",
        "\n",
        "    # --- SGD update ---\n",
        "    W1 = W1 - lr * dW1\n",
        "    b1 = b1 - lr * db1\n",
        "    W2 = W2 - lr * dW2\n",
        "    b2 = b2 - lr * db2\n",
        "\n",
        "    return W1, b1, W2, b2, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ce1d469",
      "metadata": {
        "id": "9ce1d469"
      },
      "source": [
        "## 5. Training Loop\n",
        "\n",
        "The training loop below should now work, if you did everything correctly. It does the following things:\n",
        "- Initialize the MLP weights and biases to random values\n",
        "- For every epoch:\n",
        "  - Do one SGD step you made\n",
        "  - Do one forward pass and loss computation on the test set\n",
        "  - Track the history for all weights, biases and losses, and possibly print them\n",
        "- Increment the epoch counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6198ff33",
      "metadata": {
        "id": "6198ff33"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(X_train, y_train, X_test, y_test, hidden_dim=5, lr=0.1, epochs=2000, snapshots=100):\n",
        "    W1, b1, W2, b2 = init_weights(X_train.shape[1], hidden_dim, 1)\n",
        "\n",
        "    # For later plotting results\n",
        "    history = {\"train_loss\": [], \"test_loss\": [], \"weights\": [], \"epochs\": []}\n",
        "    snapshot_epochs = np.linspace(1, epochs, snapshots, dtype=int)\n",
        "\n",
        "    # Actual training\n",
        "    for epoch in range(epochs):\n",
        "        W1, b1, W2, b2, train_loss = step(W1, b1, W2, b2, X_train, y_train, lr)\n",
        "        y_test_pred, _ = forward(X_test, W1, b1, W2, b2)\n",
        "        test_loss = compute_loss(y_test_pred, y_test)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        #store\n",
        "        if epoch in snapshot_epochs:\n",
        "            history[\"weights\"].append((W1.copy(), b1.copy(), W2.copy(), b2.copy()))\n",
        "            history[\"epochs\"].append(epoch)\n",
        "\n",
        "        if (epoch + 1) % 200 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Train Loss {train_loss:.4f}, Test Loss {test_loss:.4f}\")\n",
        "\n",
        "    return W1, b1, W2, b2, history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f781dba",
      "metadata": {
        "id": "3f781dba"
      },
      "source": [
        "## 6. Run Training\n",
        "\n",
        "**Task 7 (1 point)**: Alter the number of hidden neurons (hidden_dim) and the learning rate (lr) to decrease the loss as much as possible. Write down your results and what conclusions you would draw from them. (There is no exactly correct answer here - you observe what you observe during your experiments - just be thorough)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0e3eea",
      "metadata": {
        "id": "4b0e3eea"
      },
      "outputs": [],
      "source": [
        "W1, b1, W2, b2, history = train(X_train, y_train, X_test, y_test, hidden_dim=20, lr=0.5, epochs=2000, snapshots = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ddf18b",
      "metadata": {
        "id": "39ddf18b"
      },
      "source": [
        "Observations: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f81efc8",
      "metadata": {
        "id": "7f81efc8"
      },
      "source": [
        "## 7. Visualization of Training Dynamics\n",
        "\n",
        "Finally, since we tracked everything during the toy model training, we can look at our dataset and how intermediate versions of our model performed classification. We should observe a complicated, non-linear decision boundary that improves over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67565a7c",
      "metadata": {
        "id": "67565a7c"
      },
      "outputs": [],
      "source": [
        "# --- Animation with continuous loss + snapshot-based boundary ---\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6,8))\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "\n",
        "ax1.set_xlim(0, len(history[\"train_loss\"]))\n",
        "ax1.set_ylim(0, max(history[\"train_loss\"]+history[\"test_loss\"]))\n",
        "line1, = ax1.plot([], [], label=\"Train loss\")\n",
        "line2, = ax1.plot([], [], label=\"Test loss\")\n",
        "ax1.legend(); ax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "# Keep track of last boundary plotted\n",
        "last_boundary = {\"epoch\": None}\n",
        "\n",
        "frames = np.linspace(0, len(history[\"train_loss\"])-1, 100, dtype=int)  # only 100 frames\n",
        "\n",
        "def plot_decision_boundary(ax, X, y, W1, b1, W2, b2):\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 200),\n",
        "                         np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 200))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    probs, _ = forward(grid, W1, b1, W2, b2)\n",
        "    probs = probs.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, probs, levels=[0,0.5,1], cmap=plt.cm.coolwarm, alpha=0.6)\n",
        "    ax.scatter(X[:,0], X[:,1], c=y[:,0], cmap=plt.cm.coolwarm, edgecolors=\"k\")\n",
        "\n",
        "def init():\n",
        "    line1.set_data([], [])\n",
        "    line2.set_data([], [])\n",
        "    ax2.clear()\n",
        "    return line1, line2, ax2\n",
        "\n",
        "def update(epoch):\n",
        "    # --- Update loss curves continuously ---\n",
        "    x = np.arange(epoch+1)\n",
        "    line1.set_data(x, history[\"train_loss\"][:epoch+1])\n",
        "    line2.set_data(x, history[\"test_loss\"][:epoch+1])\n",
        "    ax1.set_title(f\"Epoch {epoch}\")\n",
        "\n",
        "    # --- Update decision boundary if we crossed a snapshot ---\n",
        "    past_snapshots = [e for e in history[\"epochs\"] if e <= epoch]\n",
        "    if past_snapshots:\n",
        "        latest_snapshot = past_snapshots[-1]\n",
        "        if last_boundary[\"epoch\"] != latest_snapshot:\n",
        "            idx = history[\"epochs\"].index(latest_snapshot)\n",
        "            ax2.clear()\n",
        "            W1, b1, W2, b2 = history[\"weights\"][idx]\n",
        "            plot_decision_boundary(ax2, X_train, y_train, W1, b1, W2, b2)\n",
        "            ax2.set_title(f\"Decision Boundary (Epoch {latest_snapshot})\")\n",
        "            last_boundary[\"epoch\"] = latest_snapshot\n",
        "\n",
        "    return line1, line2, ax2\n",
        "\n",
        "anim = FuncAnimation(\n",
        "    fig, update,\n",
        "    frames=frames,        # only every 20th epoch\n",
        "    init_func=init, blit=False\n",
        ")\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i6hOGcwpWZ5J",
      "metadata": {
        "id": "i6hOGcwpWZ5J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
